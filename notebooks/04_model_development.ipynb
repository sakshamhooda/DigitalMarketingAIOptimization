{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fb4268-f870-468f-aaa5-bd1fcf3f8ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 14:10:19.622678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310363ce-1ec5-4904-90a3-49e87e950401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "data = pd.read_csv('../data/processed/feature_engineered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee01b36e-0d0e-476e-b569-13a44a04c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Performance:\n",
      "RMSE: 18.3742\n",
      "MAE: 8.5390\n",
      "R-squared: 0.1176\n",
      "--------------------\n",
      "Random Forest Performance:\n",
      "RMSE: 28.3910\n",
      "MAE: 5.3165\n",
      "R-squared: -1.1067\n",
      "--------------------\n",
      "XGBoost Performance:\n",
      "RMSE: 137.1607\n",
      "MAE: 13.1039\n",
      "R-squared: -48.1695\n",
      "--------------------\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "Neural Network Performance:\n",
      "RMSE: 14.3990\n",
      "MAE: 3.5969\n",
      "R-squared: 0.4581\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "features = ['Impressions', 'Clicks', 'Spend', 'CTR', 'CPC', 'CVR', \n",
    "            'day_of_week', 'is_weekend', 'month', 'quarter', \n",
    "            'Spend_lag_1', 'Spend_lag_7', 'Spend_lag_30', \n",
    "            'Clicks_lag_1', 'Clicks_lag_7', 'Clicks_lag_30',\n",
    "            'Spend_rolling_7d', 'Spend_rolling_30d', \n",
    "            'Clicks_rolling_7d', 'Clicks_rolling_30d',\n",
    "            'Impressions_rolling_7d', 'Impressions_rolling_30d',\n",
    "            'Conversions_rolling_7d', 'Conversions_rolling_30d',\n",
    "            'Revenue_rolling_7d', 'Revenue_rolling_30d',\n",
    "            'CTR_vs_mean', 'CVR_vs_mean', 'Source_encoded', \n",
    "            'Campaign_type_encoded', 'is_high_spend_day']\n",
    "\n",
    "target = 'ROAS'\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define a function to evaluate models\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "# Linear Regression (baseline)\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_model.predict(X_val_scaled)\n",
    "evaluate_model(y_val, lr_pred, \"Linear Regression\")\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_val_scaled)\n",
    "evaluate_model(y_val, rf_pred, \"Random Forest\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_val_scaled)\n",
    "evaluate_model(y_val, xgb_pred, \"XGBoost\")\n",
    "\n",
    "# Neural Network\n",
    "nn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=0)\n",
    "nn_pred = nn_model.predict(X_val_scaled).flatten()\n",
    "evaluate_model(y_val, nn_pred, \"Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467482e-85ca-4d9f-b69f-829cc327f932",
   "metadata": {},
   "source": [
    "Based on these, we can see that the Neural Network model is performing the best, followed by Linear Regression. The Random Forest and XGBoost models are underperforming, which suggests they might be overfitting or need further tuning.\n",
    "\n",
    "- Implement cross-validation using TimeSeriesSplit\n",
    "- Perform hyperparameter tuning for the Neural Network and Linear Regression models\n",
    "- Conduct feature importance analysis\n",
    "- Evaluate the best model on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a29963-951c-4cb3-af06-ea884f422f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Cross-Validation Scores: [-0.29102876560856594, -1.2168742882080883, 0.43280929426266135, 0.48054674036703116, 0.20436899359622762]\n",
      "Mean R-squared: -0.0780\n",
      "--------------------\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 6ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "Best Neural Network Parameters:\n",
      "{'neurons': 128, 'dropout_rate': 0.1, 'learning_rate': 0.01}\n",
      "Best RMSE: 13.3114\n",
      "--------------------\n",
      "Top 10 Most Important Features:\n",
      "                   Feature  Importance\n",
      "19      Clicks_rolling_30d    0.087474\n",
      "18       Clicks_rolling_7d    0.059165\n",
      "17       Spend_rolling_30d    0.058906\n",
      "28          Source_encoded    0.050993\n",
      "24      Revenue_rolling_7d    0.050464\n",
      "20  Impressions_rolling_7d    0.049809\n",
      "13            Clicks_lag_1    0.045962\n",
      "16        Spend_rolling_7d    0.044213\n",
      "1                   Clicks    0.041158\n",
      "10             Spend_lag_1    0.040778\n",
      "--------------------\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "Best Neural Network (Test Set) Performance:\n",
      "RMSE: 10.4508\n",
      "MAE: 3.6500\n",
      "R-squared: 0.6219\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Load the preprocessed data\n",
    "data = pd.read_csv('../data/processed/feature_engineered_data.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Define features and target\n",
    "features = ['Impressions', 'Clicks', 'Spend', 'CTR', 'CPC', 'CVR', \n",
    "            'day_of_week', 'is_weekend', 'month', 'quarter', \n",
    "            'Spend_lag_1', 'Spend_lag_7', 'Spend_lag_30', \n",
    "            'Clicks_lag_1', 'Clicks_lag_7', 'Clicks_lag_30',\n",
    "            'Spend_rolling_7d', 'Spend_rolling_30d', \n",
    "            'Clicks_rolling_7d', 'Clicks_rolling_30d',\n",
    "            'Impressions_rolling_7d', 'Impressions_rolling_30d',\n",
    "            'Conversions_rolling_7d', 'Conversions_rolling_30d',\n",
    "            'Revenue_rolling_7d', 'Revenue_rolling_30d',\n",
    "            'CTR_vs_mean', 'CVR_vs_mean', 'Source_encoded', \n",
    "            'Campaign_type_encoded', 'is_high_spend_day']\n",
    "\n",
    "target = 'ROAS'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "# Linear Regression with cross-validation\n",
    "lr_model = LinearRegression()\n",
    "lr_scores = []\n",
    "\n",
    "for train_index, val_index in tscv.split(X_scaled):\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    lr_model.fit(X_train, y_train)\n",
    "    lr_pred = lr_model.predict(X_val)\n",
    "    lr_scores.append(r2_score(y_val, lr_pred))\n",
    "\n",
    "print(f\"Linear Regression Cross-Validation Scores: {lr_scores}\")\n",
    "print(f\"Mean R-squared: {np.mean(lr_scores):.4f}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# Neural Network with manual hyperparameter tuning\n",
    "def create_model(neurons, dropout_rate, learning_rate):\n",
    "    model = Sequential([\n",
    "        Dense(neurons, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(neurons // 2, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(neurons // 4, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Hyperparameter options\n",
    "param_options = {\n",
    "    'neurons': [32, 64, 128],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for neurons in param_options['neurons']:\n",
    "    for dropout_rate in param_options['dropout_rate']:\n",
    "        for learning_rate in param_options['learning_rate']:\n",
    "            nn_scores = []\n",
    "            for train_index, val_index in tscv.split(X_scaled):\n",
    "                X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "                y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                \n",
    "                model = create_model(neurons, dropout_rate, learning_rate)\n",
    "                model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0, validation_data=(X_val, y_val))\n",
    "                \n",
    "                y_pred = model.predict(X_val).flatten()\n",
    "                mse = mean_squared_error(y_val, y_pred)\n",
    "                nn_scores.append(mse)\n",
    "            \n",
    "            mean_mse = np.mean(nn_scores)\n",
    "            if mean_mse < best_score:\n",
    "                best_score = mean_mse\n",
    "                best_params = {'neurons': neurons, 'dropout_rate': dropout_rate, 'learning_rate': learning_rate}\n",
    "\n",
    "print(\"Best Neural Network Parameters:\")\n",
    "print(best_params)\n",
    "print(f\"Best RMSE: {np.sqrt(best_score):.4f}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# Feature Importance Analysis (using the best Neural Network model)\n",
    "best_nn_model = create_model(**best_params)\n",
    "best_nn_model.fit(X_scaled, y, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "feature_importance = np.abs(best_nn_model.layers[0].get_weights()[0].mean(axis=1))\n",
    "feature_importance = feature_importance / np.sum(feature_importance)\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance_df.head(10))\n",
    "print(\"--------------------\")\n",
    "\n",
    "# Evaluate best model on test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
    "best_nn_model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "y_pred = best_nn_model.predict(X_test).flatten()\n",
    "\n",
    "evaluate_model(y_test, y_pred, \"Best Neural Network (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e86b73-1812-4b22-b30a-2fcb238c4931",
   "metadata": {},
   "source": [
    "Key Insights:\n",
    "\n",
    "1. The neural network outperforms linear regression, suggesting non-linear relationships in the data.\n",
    "2. Rolling metrics (7-day and 30-day) are crucial for predicting ROAS, indicating the importance of recent trends.\n",
    "3. The model explains about 62% of the variance in ROAS (R-squared of 0.6219), which is good but leaves room for improvement.\n",
    "4. The Source_encoded feature is important, suggesting that different ad platforms have varying impacts on ROAS.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "1. Feature Engineering: Create more complex features, especially focusing on interactions between top features.\n",
    "2. Ensemble Methods: Try combining multiple models (e.g., Random Forest, XGBoost) with the neural network.\n",
    "3. Advanced Architectures: Experiment with more complex neural network architectures, possibly including LSTM layers to capture temporal dependencies better.\n",
    "4. Regularization: Implement regularization techniques to prevent overfitting and potentially improve generalization.\n",
    "5. Anomaly Detection: Investigate outliers or anomalous periods in the data that might be affecting model performance.\n",
    "6. Segmentation: Consider creating separate models for different ad sources or campaign types if they exhibit significantly different patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5a18ec-7f15-4733-afc8-fe559756b4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 10:21:05.949281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_8239/2879551792.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Click_Spend_Ratio'] = X['Clicks'] / (X['Spend'] + 1)  # Adding 1 to avoid division by zero\n",
      "/tmp/ipykernel_8239/2879551792.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Revenue_per_Click'] = data['Revenue'] / (X['Clicks'] + 1)\n",
      "/tmp/ipykernel_8239/2879551792.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Spend_Acceleration'] = X['Spend_rolling_7d'] - X['Spend_rolling_30d'] / 30 * 7\n",
      "/tmp/ipykernel_8239/2879551792.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['Click_Acceleration'] = X['Clicks_rolling_7d'] - X['Clicks_rolling_30d'] / 30 * 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance:\n",
      "RMSE: 6.2354\n",
      "MAE: 1.9500\n",
      "R-squared: 0.8654\n",
      "--------------------\n",
      "XGBoost Performance:\n",
      "RMSE: 3.2188\n",
      "MAE: 1.3627\n",
      "R-squared: 0.9641\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step\n",
      "LSTM Neural Network Performance:\n",
      "RMSE: 3.5239\n",
      "MAE: 2.3496\n",
      "R-squared: 0.9570\n",
      "--------------------\n",
      "Ensemble Model Performance:\n",
      "RMSE: 3.1279\n",
      "MAE: 1.4910\n",
      "R-squared: 0.9661\n",
      "--------------------\n",
      "Top 15 Most Important Features (Random Forest):\n",
      "                    feature  importance\n",
      "5                       CVR    0.349262\n",
      "27              CVR_vs_mean    0.238863\n",
      "32        Revenue_per_Click    0.201282\n",
      "0               Impressions    0.093412\n",
      "4                       CPC    0.040365\n",
      "17        Spend_rolling_30d    0.024994\n",
      "2                     Spend    0.014764\n",
      "31        Click_Spend_Ratio    0.007553\n",
      "25      Revenue_rolling_30d    0.004006\n",
      "34       Click_Acceleration    0.002602\n",
      "23  Conversions_rolling_30d    0.002353\n",
      "16         Spend_rolling_7d    0.002338\n",
      "28           Source_encoded    0.001639\n",
      "14             Clicks_lag_7    0.001581\n",
      "19       Clicks_rolling_30d    0.001502\n",
      "--------------------\n",
      "Performance for Microsoft Ads:\n",
      "Microsoft Ads Performance:\n",
      "RMSE: 2.7395\n",
      "MAE: 1.4005\n",
      "R-squared: 0.9619\n",
      "--------------------\n",
      "Performance for Meta Ads:\n",
      "Meta Ads Performance:\n",
      "RMSE: 5.0728\n",
      "MAE: 3.3461\n",
      "R-squared: 0.9624\n",
      "--------------------\n",
      "Performance for Google Ads:\n",
      "Google Ads Performance:\n",
      "RMSE: 2.7602\n",
      "MAE: 1.0018\n",
      "R-squared: -0.6434\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load and preprocess data (assuming this step is already done)\n",
    "data = pd.read_csv('../data/processed/feature_engineered_data.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Define features and target\n",
    "features = ['Impressions', 'Clicks', 'Spend', 'CTR', 'CPC', 'CVR', \n",
    "            'day_of_week', 'is_weekend', 'month', 'quarter', \n",
    "            'Spend_lag_1', 'Spend_lag_7', 'Spend_lag_30', \n",
    "            'Clicks_lag_1', 'Clicks_lag_7', 'Clicks_lag_30',\n",
    "            'Spend_rolling_7d', 'Spend_rolling_30d', \n",
    "            'Clicks_rolling_7d', 'Clicks_rolling_30d',\n",
    "            'Impressions_rolling_7d', 'Impressions_rolling_30d',\n",
    "            'Conversions_rolling_7d', 'Conversions_rolling_30d',\n",
    "            'Revenue_rolling_7d', 'Revenue_rolling_30d',\n",
    "            'CTR_vs_mean', 'CVR_vs_mean', 'Source_encoded', \n",
    "            'Campaign_type_encoded', 'is_high_spend_day']\n",
    "\n",
    "target = 'ROAS'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Feature Engineering\n",
    "X['Click_Spend_Ratio'] = X['Clicks'] / (X['Spend'] + 1)  # Adding 1 to avoid division by zero\n",
    "X['Revenue_per_Click'] = data['Revenue'] / (X['Clicks'] + 1)\n",
    "X['Spend_Acceleration'] = X['Spend_rolling_7d'] - X['Spend_rolling_30d'] / 30 * 7\n",
    "X['Click_Acceleration'] = X['Clicks_rolling_7d'] - X['Clicks_rolling_30d'] / 30 * 7\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Time Series Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "evaluate_model(y_test, rf_pred, \"Random Forest\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "evaluate_model(y_test, xgb_pred, \"XGBoost\")\n",
    "\n",
    "# Advanced Neural Network with LSTM and Regularization\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(input_shape[1], input_shape[2]), return_sequences=True, \n",
    "             kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Reshape input for LSTM (samples, time steps, features)\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "lstm_model = create_lstm_model(X_train_lstm.shape)\n",
    "lstm_model.fit(X_train_lstm, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "lstm_pred = lstm_model.predict(X_test_lstm).flatten()\n",
    "evaluate_model(y_test, lstm_pred, \"LSTM Neural Network\")\n",
    "\n",
    "# Ensemble (Simple average of RF, XGB, and LSTM predictions)\n",
    "ensemble_pred = (rf_pred + xgb_pred + lstm_pred) / 3\n",
    "evaluate_model(y_test, ensemble_pred, \"Ensemble Model\")\n",
    "\n",
    "# Feature Importance (using Random Forest)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Most Important Features (Random Forest):\")\n",
    "print(feature_importance.head(15))\n",
    "print(\"--------------------\")\n",
    "\n",
    "# Analyze model performance across different ad sources\n",
    "source_performance = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': ensemble_pred,\n",
    "    'Source': data.loc[y_test.index, 'Source']\n",
    "})\n",
    "\n",
    "for source in source_performance['Source'].unique():\n",
    "    source_data = source_performance[source_performance['Source'] == source]\n",
    "    print(f\"Performance for {source}:\")\n",
    "    evaluate_model(source_data['Actual'], source_data['Predicted'], source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df86189-4614-4426-8083-796e38040d36",
   "metadata": {},
   "source": [
    "Key Insights and Next Steps:\n",
    "\n",
    "1. Model Selection: The Ensemble Model performs best, but XGBoost is nearly as good and might be preferred for its simplicity and interpretability.\n",
    "2. Feature Focus: Prioritize efforts on improving CVR, understanding how campaigns compare to average CVR, and optimizing Revenue per Click.\n",
    "3. Google Ads Performance: Investigate why the model performs poorly for Google Ads. Possible reasons:\n",
    "   - Different dynamics or patterns in Google Ads data\n",
    "   - Insufficient or unrepresentative Google Ads data in the training set\n",
    "   -Presence of outliers or anomalies in Google Ads data\n",
    "4. Meta Ads: While the R-squared is high, the RMSE is higher than for other platforms. This suggests more volatile or harder-to-predict ROAS for Meta Ads.\n",
    "5. Feature Engineering: Consider creating more features related to CVR and revenue, as these seem to be highly predictive.\n",
    "6. Hyperparameter Tuning: Fine-tune the XGBoost and Random Forest models to potentially improve performance further.\n",
    "7. Time-based Analysis: Investigate if there are seasonal trends or time-based patterns that could be incorporated into the model.\n",
    "8. Segmentation: Consider building separate models for each ad platform, especially given the poor performance on Google Ads.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "1. Dive deeper into the Google Ads data to understand why the model is underperforming.\n",
    "2. Experiment with platform-specific models.\n",
    "3. Create additional features based on the top important features identified.\n",
    "4. Implement cross-validation for more robust performance estimates.\n",
    "5. Develop a strategy for real-time or periodic model updates as new data becomes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb864982-d05d-411a-be83-77a5b31f2bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Google Ads\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Click_Spend_Ratio', 'Revenue_per_Click', 'Spend_Acceleration', 'Click_Acceleration'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalyzing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplatform\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m platform_df \u001b[38;5;241m=\u001b[39m platform_data[platform]\n\u001b[0;32m---> 77\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mplatform_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     78\u001b[0m y \u001b[38;5;241m=\u001b[39m platform_df[target]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Scale features\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Click_Spend_Ratio', 'Revenue_per_Click', 'Spend_Acceleration', 'Click_Acceleration'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('../data/processed/feature_engineered_data.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Define features and target\n",
    "features = ['Impressions', 'Clicks', 'Spend', 'CTR', 'CPC', 'CVR', \n",
    "            'day_of_week', 'is_weekend', 'month', 'quarter', \n",
    "            'Spend_lag_1', 'Spend_lag_7', 'Spend_lag_30', \n",
    "            'Clicks_lag_1', 'Clicks_lag_7', 'Clicks_lag_30',\n",
    "            'Spend_rolling_7d', 'Spend_rolling_30d', \n",
    "            'Clicks_rolling_7d', 'Clicks_rolling_30d',\n",
    "            'Impressions_rolling_7d', 'Impressions_rolling_30d',\n",
    "            'Conversions_rolling_7d', 'Conversions_rolling_30d',\n",
    "            'Revenue_rolling_7d', 'Revenue_rolling_30d',\n",
    "            'CTR_vs_mean', 'CVR_vs_mean', 'Source_encoded', \n",
    "            'Campaign_type_encoded', 'is_high_spend_day',\n",
    "            'Click_Spend_Ratio', 'Revenue_per_Click', \n",
    "            'Spend_Acceleration', 'Click_Acceleration']\n",
    "\n",
    "target = 'ROAS'\n",
    "\n",
    "# Feature Engineering\n",
    "data['CVR_rolling_7d'] = data['Conversions_rolling_7d'] / data['Clicks_rolling_7d']\n",
    "data['CVR_rolling_30d'] = data['Conversions_rolling_30d'] / data['Clicks_rolling_30d']\n",
    "data['Revenue_per_Impression'] = data['Revenue'] / data['Impressions']\n",
    "features += ['CVR_rolling_7d', 'CVR_rolling_30d', 'Revenue_per_Impression']\n",
    "\n",
    "# Separate data by platform\n",
    "platforms = ['Google Ads', 'Microsoft Ads', 'Meta Ads']\n",
    "platform_data = {platform: data[data['Source'] == platform] for platform in platforms}\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "# XGBoost model with cross-validation\n",
    "def train_xgboost_with_cv(X, y, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    \n",
    "    cv_scores = []\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        cv_scores.append(r2_score(y_val, y_pred))\n",
    "    \n",
    "    print(f\"Cross-validation R-squared scores: {cv_scores}\")\n",
    "    print(f\"Mean R-squared: {np.mean(cv_scores):.4f}\")\n",
    "    \n",
    "    return xgb_model.fit(X, y)\n",
    "\n",
    "# Train and evaluate models for each platform\n",
    "for platform in platforms:\n",
    "    print(f\"\\nAnalyzing {platform}\")\n",
    "    platform_df = platform_data[platform]\n",
    "    X = platform_df[features]\n",
    "    y = platform_df[target]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Train XGBoost model with cross-validation\n",
    "    xgb_model = train_xgboost_with_cv(X_scaled, y)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Important Features for {platform}:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Final evaluation on the entire dataset\n",
    "    y_pred = xgb_model.predict(X_scaled)\n",
    "    evaluate_model(y, y_pred, f\"XGBoost for {platform}\")\n",
    "\n",
    "# Analyze Google Ads data in more detail\n",
    "google_ads_data = platform_data['Google Ads']\n",
    "print(\"\\nGoogle Ads Data Analysis:\")\n",
    "print(google_ads_data[['ROAS', 'CVR', 'Revenue_per_Click', 'Impressions']].describe())\n",
    "\n",
    "# Check for outliers in Google Ads ROAS\n",
    "q1 = google_ads_data['ROAS'].quantile(0.25)\n",
    "q3 = google_ads_data['ROAS'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "outliers = google_ads_data[(google_ads_data['ROAS'] < lower_bound) | (google_ads_data['ROAS'] > upper_bound)]\n",
    "print(f\"\\nNumber of outliers in Google Ads ROAS: {len(outliers)}\")\n",
    "print(\"Sample of outliers:\")\n",
    "print(outliers[['Date', 'ROAS', 'CVR', 'Revenue_per_Click', 'Impressions']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5ee176-47c8-4e7c-8e95-248274aac114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Google Ads\n",
      "Cross-validation R-squared scores: [0.9375084268094541, 0.9545132863021434, 0.9685334723533959, 0.8703949127935939, 0.9550621621331513]\n",
      "Mean R-squared: 0.9372\n",
      "\n",
      "Top 10 Important Features for Google Ads:\n",
      "                    feature  importance\n",
      "32        Revenue_per_Click    0.606541\n",
      "37   Revenue_per_Impression    0.264844\n",
      "4                       CPC    0.043288\n",
      "8                     month    0.029092\n",
      "0               Impressions    0.011762\n",
      "20   Impressions_rolling_7d    0.009954\n",
      "1                    Clicks    0.008162\n",
      "31        Click_Spend_Ratio    0.006530\n",
      "21  Impressions_rolling_30d    0.005207\n",
      "16         Spend_rolling_7d    0.003772\n",
      "XGBoost for Google Ads Performance:\n",
      "RMSE: 0.0009\n",
      "MAE: 0.0006\n",
      "R-squared: 1.0000\n",
      "--------------------\n",
      "\n",
      "Analyzing Microsoft Ads\n",
      "Cross-validation R-squared scores: [0.7748007025165602, 0.9620580080049659, 0.7533612091165727, 0.946754795683404, 0.5069591825311797]\n",
      "Mean R-squared: 0.7888\n",
      "\n",
      "Top 10 Important Features for Microsoft Ads:\n",
      "                   feature  importance\n",
      "0              Impressions    0.937813\n",
      "32       Revenue_per_Click    0.056290\n",
      "2                    Spend    0.002523\n",
      "4                      CPC    0.001559\n",
      "37  Revenue_per_Impression    0.000472\n",
      "31       Click_Spend_Ratio    0.000413\n",
      "10             Spend_lag_1    0.000252\n",
      "35          CVR_rolling_7d    0.000245\n",
      "24      Revenue_rolling_7d    0.000094\n",
      "29   Campaign_type_encoded    0.000086\n",
      "XGBoost for Microsoft Ads Performance:\n",
      "RMSE: 0.0068\n",
      "MAE: 0.0048\n",
      "R-squared: 1.0000\n",
      "--------------------\n",
      "\n",
      "Analyzing Meta Ads\n",
      "Cross-validation R-squared scores: [0.41071066873335726, 0.8757719551502731, 0.5903019195184402, -0.32125466783713597, 0.9237575011385057]\n",
      "Mean R-squared: 0.4959\n",
      "\n",
      "Top 10 Important Features for Meta Ads:\n",
      "                    feature  importance\n",
      "37   Revenue_per_Impression    0.774225\n",
      "32        Revenue_per_Click    0.130403\n",
      "19       Clicks_rolling_30d    0.033193\n",
      "0               Impressions    0.026037\n",
      "2                     Spend    0.006397\n",
      "18        Clicks_rolling_7d    0.005579\n",
      "21  Impressions_rolling_30d    0.005437\n",
      "4                       CPC    0.004276\n",
      "24       Revenue_rolling_7d    0.003580\n",
      "1                    Clicks    0.002687\n",
      "XGBoost for Meta Ads Performance:\n",
      "RMSE: 0.0007\n",
      "MAE: 0.0005\n",
      "R-squared: 1.0000\n",
      "--------------------\n",
      "\n",
      "Google Ads Data Analysis:\n",
      "             ROAS         CVR  Revenue_per_Click    Impressions\n",
      "count  512.000000  512.000000         512.000000     512.000000\n",
      "mean     2.886467    0.033531           7.953671  140731.832031\n",
      "std      2.783161    0.052413          25.684567  187229.469538\n",
      "min      0.000000    0.000000           0.000000      30.000000\n",
      "25%      0.000000    0.000000           0.000000    9621.250000\n",
      "50%      2.665600    0.022557           4.425315   19960.500000\n",
      "75%      4.296583    0.051746          10.596878  328324.250000\n",
      "max     22.023622    0.800000         559.400000  698237.000000\n",
      "\n",
      "Number of outliers in Google Ads ROAS: 5\n",
      "Sample of outliers:\n",
      "          Date       ROAS       CVR  Revenue_per_Click  Impressions\n",
      "0   2024-01-08  12.010340  0.124943          25.989498       3655.0\n",
      "43  2024-01-23  12.930026  0.098101          33.191153       9025.0\n",
      "80  2024-02-04  22.023622  0.400000         559.400000      21836.0\n",
      "392 2024-05-22  11.025860  0.115764          40.018182       5836.0\n",
      "413 2024-05-29  12.768911  0.114286          49.886992       4455.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('../data/processed/feature_engineered_data.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Feature Engineering\n",
    "data['Click_Spend_Ratio'] = data['Clicks'] / (data['Spend'] + 1)  # Adding 1 to avoid division by zero\n",
    "data['Revenue_per_Click'] = data['Revenue'] / (data['Clicks'] + 1)\n",
    "data['Spend_Acceleration'] = data['Spend_rolling_7d'] - data['Spend_rolling_30d'] / 30 * 7\n",
    "data['Click_Acceleration'] = data['Clicks_rolling_7d'] - data['Clicks_rolling_30d'] / 30 * 7\n",
    "data['CVR_rolling_7d'] = data['Conversions_rolling_7d'] / (data['Clicks_rolling_7d'] + 1)\n",
    "data['CVR_rolling_30d'] = data['Conversions_rolling_30d'] / (data['Clicks_rolling_30d'] + 1)\n",
    "data['Revenue_per_Impression'] = data['Revenue'] / (data['Impressions'] + 1)\n",
    "\n",
    "# Define features and target\n",
    "features = ['Impressions', 'Clicks', 'Spend', 'CTR', 'CPC', 'CVR', \n",
    "            'day_of_week', 'is_weekend', 'month', 'quarter', \n",
    "            'Spend_lag_1', 'Spend_lag_7', 'Spend_lag_30', \n",
    "            'Clicks_lag_1', 'Clicks_lag_7', 'Clicks_lag_30',\n",
    "            'Spend_rolling_7d', 'Spend_rolling_30d', \n",
    "            'Clicks_rolling_7d', 'Clicks_rolling_30d',\n",
    "            'Impressions_rolling_7d', 'Impressions_rolling_30d',\n",
    "            'Conversions_rolling_7d', 'Conversions_rolling_30d',\n",
    "            'Revenue_rolling_7d', 'Revenue_rolling_30d',\n",
    "            'CTR_vs_mean', 'CVR_vs_mean', 'Source_encoded', \n",
    "            'Campaign_type_encoded', 'is_high_spend_day',\n",
    "            'Click_Spend_Ratio', 'Revenue_per_Click', \n",
    "            'Spend_Acceleration', 'Click_Acceleration',\n",
    "            'CVR_rolling_7d', 'CVR_rolling_30d', 'Revenue_per_Impression']\n",
    "\n",
    "target = 'ROAS'\n",
    "\n",
    "# Separate data by platform\n",
    "platforms = ['Google Ads', 'Microsoft Ads', 'Meta Ads']\n",
    "platform_data = {platform: data[data['Source'] == platform] for platform in platforms}\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "# XGBoost model with cross-validation\n",
    "def train_xgboost_with_cv(X, y, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    \n",
    "    cv_scores = []\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        cv_scores.append(r2_score(y_val, y_pred))\n",
    "    \n",
    "    print(f\"Cross-validation R-squared scores: {cv_scores}\")\n",
    "    print(f\"Mean R-squared: {np.mean(cv_scores):.4f}\")\n",
    "    \n",
    "    return xgb_model.fit(X, y)\n",
    "\n",
    "# Train and evaluate models for each platform\n",
    "for platform in platforms:\n",
    "    print(f\"\\nAnalyzing {platform}\")\n",
    "    platform_df = platform_data[platform]\n",
    "    X = platform_df[features]\n",
    "    y = platform_df[target]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Train XGBoost model with cross-validation\n",
    "    xgb_model = train_xgboost_with_cv(X_scaled, y)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Important Features for {platform}:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Final evaluation on the entire dataset\n",
    "    y_pred = xgb_model.predict(X_scaled)\n",
    "    evaluate_model(y, y_pred, f\"XGBoost for {platform}\")\n",
    "\n",
    "# Analyze Google Ads data in more detail\n",
    "google_ads_data = platform_data['Google Ads']\n",
    "print(\"\\nGoogle Ads Data Analysis:\")\n",
    "print(google_ads_data[['ROAS', 'CVR', 'Revenue_per_Click', 'Impressions']].describe())\n",
    "\n",
    "# Check for outliers in Google Ads ROAS\n",
    "q1 = google_ads_data['ROAS'].quantile(0.25)\n",
    "q3 = google_ads_data['ROAS'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "outliers = google_ads_data[(google_ads_data['ROAS'] < lower_bound) | (google_ads_data['ROAS'] > upper_bound)]\n",
    "print(f\"\\nNumber of outliers in Google Ads ROAS: {len(outliers)}\")\n",
    "print(\"Sample of outliers:\")\n",
    "print(outliers[['Date', 'ROAS', 'CVR', 'Revenue_per_Click', 'Impressions']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d5e64-7b2a-4dba-a9f8-98c8e761c30d",
   "metadata": {},
   "source": [
    "Key Insights:\n",
    "\n",
    "All platforms show signs of overfitting, with perfect R-squared scores on the entire dataset. This indicates that our models might be too complex or that we need more regularization.\n",
    "Google Ads performance is much better than initially thought, with high cross-validation scores.\n",
    "Revenue-related features (Revenue_per_Click, Revenue_per_Impression) are consistently important across all platforms.\n",
    "The importance of features varies significantly between platforms, suggesting that platform-specific models are indeed beneficial.\n",
    "Meta Ads show the most inconsistent performance across cross-validation folds, indicating potential issues with data variability or temporal patterns.\n",
    "Google Ads have a few significant outliers, with ROAS values much higher than the average.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "Address overfitting:\n",
    "\n",
    "Implement regularization in XGBoost (adjust alpha and lambda parameters)\n",
    "Reduce model complexity (decrease max_depth, min_child_weight)\n",
    "Increase min_samples_leaf to force the model to generalize better\n",
    "\n",
    "\n",
    "Handle outliers in Google Ads data:\n",
    "\n",
    "Investigate the outlier data points to understand if they are errors or genuine high-performing campaigns\n",
    "Consider using robust scaling or winsorization to reduce the impact of outliers\n",
    "\n",
    "\n",
    "Improve Meta Ads performance:\n",
    "\n",
    "Investigate temporal patterns in Meta Ads data\n",
    "Consider using a more sophisticated time series model (e.g., ARIMA, Prophet) for Meta Ads\n",
    "\n",
    "\n",
    "Feature engineering:\n",
    "\n",
    "Create interaction features between top important features\n",
    "Develop platform-specific features based on the importance rankings\n",
    "\n",
    "\n",
    "Ensemble modeling:\n",
    "\n",
    "Combine XGBoost with other algorithms (e.g., LightGBM, CatBoost) to create a more robust ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a353c2-fe4e-4d05-924e-42ab2d8adaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Google Ads\n",
      "Cross-validation R-squared scores: [0.9375084268094541, 0.9545172258298527, 0.9685331916020812, 0.8696005397955601, 0.9550630940720943]\n",
      "Mean R-squared: 0.9370\n",
      "\n",
      "Top 10 Important Features for Google Ads:\n",
      "                    feature  importance\n",
      "32        Revenue_per_Click    0.606541\n",
      "37   Revenue_per_Impression    0.264844\n",
      "4                       CPC    0.043288\n",
      "8                     month    0.029092\n",
      "0               Impressions    0.011762\n",
      "20   Impressions_rolling_7d    0.009954\n",
      "1                    Clicks    0.008162\n",
      "31        Click_Spend_Ratio    0.006530\n",
      "21  Impressions_rolling_30d    0.005207\n",
      "16         Spend_rolling_7d    0.003772\n",
      "XGBoost for Google Ads Performance:\n",
      "RMSE: 0.0009\n",
      "MAE: 0.0006\n",
      "R-squared: 1.0000\n",
      "--------------------\n",
      "\n",
      "Analyzing Microsoft Ads\n",
      "Cross-validation R-squared scores: [0.7748412426121468, 0.9620580079788191, 0.7533622196453772, 0.946755563527272, 0.5069592052153135]\n",
      "Mean R-squared: 0.7888\n",
      "\n",
      "Top 10 Important Features for Microsoft Ads:\n",
      "                   feature  importance\n",
      "0              Impressions    0.937813\n",
      "32       Revenue_per_Click    0.056290\n",
      "2                    Spend    0.002523\n",
      "4                      CPC    0.001559\n",
      "37  Revenue_per_Impression    0.000472\n",
      "31       Click_Spend_Ratio    0.000413\n",
      "10             Spend_lag_1    0.000252\n",
      "35          CVR_rolling_7d    0.000245\n",
      "24      Revenue_rolling_7d    0.000094\n",
      "29   Campaign_type_encoded    0.000086\n",
      "XGBoost for Microsoft Ads Performance:\n",
      "RMSE: 0.0068\n",
      "MAE: 0.0048\n",
      "R-squared: 1.0000\n",
      "--------------------\n",
      "\n",
      "Analyzing Meta Ads\n",
      "Cross-validation R-squared scores: [0.41068825149778976, 0.8757719551502731, 0.5903017296335895, -0.32125466783713597, 0.9237575011385057]\n",
      "Mean R-squared: 0.4959\n",
      "\n",
      "Top 10 Important Features for Meta Ads:\n",
      "                    feature  importance\n",
      "37   Revenue_per_Impression    0.774225\n",
      "32        Revenue_per_Click    0.130403\n",
      "19       Clicks_rolling_30d    0.033193\n",
      "0               Impressions    0.026037\n",
      "2                     Spend    0.006397\n",
      "18        Clicks_rolling_7d    0.005579\n",
      "21  Impressions_rolling_30d    0.005437\n",
      "4                       CPC    0.004276\n",
      "24       Revenue_rolling_7d    0.003580\n",
      "1                    Clicks    0.002687\n",
      "XGBoost for Meta Ads Performance:\n",
      "RMSE: 0.0007\n",
      "MAE: 0.0005\n",
      "R-squared: 1.0000\n",
      "--------------------\n",
      "\n",
      "Google Ads Data Analysis:\n",
      "             ROAS         CVR  Revenue_per_Click    Impressions\n",
      "count  512.000000  512.000000         512.000000     512.000000\n",
      "mean     2.886467    0.033531           7.953671  140731.832031\n",
      "std      2.783161    0.052413          25.684567  187229.469538\n",
      "min      0.000000    0.000000           0.000000      30.000000\n",
      "25%      0.000000    0.000000           0.000000    9621.250000\n",
      "50%      2.665600    0.022557           4.425315   19960.500000\n",
      "75%      4.296583    0.051746          10.596878  328324.250000\n",
      "max     22.023622    0.800000         559.400000  698237.000000\n",
      "\n",
      "Number of outliers in Google Ads ROAS: 5\n",
      "Sample of outliers:\n",
      "          Date       ROAS       CVR  Revenue_per_Click  Impressions\n",
      "0   2024-01-08  12.010340  0.124943          25.989498       3655.0\n",
      "43  2024-01-23  12.930026  0.098101          33.191153       9025.0\n",
      "80  2024-02-04  22.023622  0.400000         559.400000      21836.0\n",
      "392 2024-05-22  11.025860  0.115764          40.018182       5836.0\n",
      "413 2024-05-29  12.768911  0.114286          49.886992       4455.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('../data/processed/feature_engineered_data.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Feature Engineering\n",
    "data['Click_Spend_Ratio'] = data['Clicks'] / (data['Spend'] + 1)  # Adding 1 to avoid division by zero\n",
    "data['Revenue_per_Click'] = data['Revenue'] / (data['Clicks'] + 1)\n",
    "data['Spend_Acceleration'] = data['Spend_rolling_7d'] - data['Spend_rolling_30d'] / 30 * 7\n",
    "data['Click_Acceleration'] = data['Clicks_rolling_7d'] - data['Clicks_rolling_30d'] / 30 * 7\n",
    "data['CVR_rolling_7d'] = data['Conversions_rolling_7d'] / (data['Clicks_rolling_7d'] + 1)\n",
    "data['CVR_rolling_30d'] = data['Conversions_rolling_30d'] / (data['Clicks_rolling_30d'] + 1)\n",
    "data['Revenue_per_Impression'] = data['Revenue'] / (data['Impressions'] + 1)\n",
    "\n",
    "# Define features and target\n",
    "features = ['Impressions', 'Clicks', 'Spend', 'CTR', 'CPC', 'CVR', \n",
    "            'day_of_week', 'is_weekend', 'month', 'quarter', \n",
    "            'Spend_lag_1', 'Spend_lag_7', 'Spend_lag_30', \n",
    "            'Clicks_lag_1', 'Clicks_lag_7', 'Clicks_lag_30',\n",
    "            'Spend_rolling_7d', 'Spend_rolling_30d', \n",
    "            'Clicks_rolling_7d', 'Clicks_rolling_30d',\n",
    "            'Impressions_rolling_7d', 'Impressions_rolling_30d',\n",
    "            'Conversions_rolling_7d', 'Conversions_rolling_30d',\n",
    "            'Revenue_rolling_7d', 'Revenue_rolling_30d',\n",
    "            'CTR_vs_mean', 'CVR_vs_mean', 'Source_encoded', \n",
    "            'Campaign_type_encoded', 'is_high_spend_day',\n",
    "            'Click_Spend_Ratio', 'Revenue_per_Click', \n",
    "            'Spend_Acceleration', 'Click_Acceleration',\n",
    "            'CVR_rolling_7d', 'CVR_rolling_30d', 'Revenue_per_Impression']\n",
    "\n",
    "target = 'ROAS'\n",
    "\n",
    "# Separate data by platform\n",
    "platforms = ['Google Ads', 'Microsoft Ads', 'Meta Ads']\n",
    "platform_data = {platform: data[data['Source'] == platform] for platform in platforms}\n",
    "\n",
    "# Function to handle outliers using winsorization\n",
    "def winsorize(s, limits=(0.05, 0.05)):\n",
    "    return s.clip(lower=s.quantile(limits[0]), upper=s.quantile(1 - limits[1]))\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "# Updated XGBoost model with regularization and reduced complexity\n",
    "def create_xgb_model():\n",
    "    return XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eta=0.1,\n",
    "        alpha=1,\n",
    "        reg_lambda=1,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "#def train_xgboost_with_cv(X, y, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    xgb_model = create_xgb_model()\n",
    "    \n",
    "    cv_scores = []\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        # Apply winsorization to y_train\n",
    "        y_train = winsorize(y_train)\n",
    "        \n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        cv_scores.append(r2_score(y_val, y_pred))\n",
    "    \n",
    "    print(f\"Cross-validation R-squared scores: {cv_scores}\")\n",
    "    print(f\"Mean R-squared: {np.mean(cv_scores):.4f}\")\n",
    "    \n",
    "    return xgb_model.fit(X, winsorize(y))\n",
    "\n",
    "# Train and evaluate models for each platform\n",
    "for platform in platforms:\n",
    "    print(f\"\\nAnalyzing {platform}\")\n",
    "    platform_df = platform_data[platform]\n",
    "    X = platform_df[features]\n",
    "    y = platform_df[target]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Train XGBoost model with cross-validation\n",
    "    xgb_model = train_xgboost_with_cv(X_scaled, y)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Important Features for {platform}:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Final evaluation on the entire dataset\n",
    "    y_pred = xgb_model.predict(X_scaled)\n",
    "    evaluate_model(y, y_pred, f\"XGBoost for {platform}\")\n",
    "\n",
    "# Analyze Google Ads data in more detail\n",
    "google_ads_data = platform_data['Google Ads']\n",
    "print(\"\\nGoogle Ads Data Analysis:\")\n",
    "print(google_ads_data[['ROAS', 'CVR', 'Revenue_per_Click', 'Impressions']].describe())\n",
    "\n",
    "# Check for outliers in Google Ads ROAS\n",
    "q1 = google_ads_data['ROAS'].quantile(0.25)\n",
    "q3 = google_ads_data['ROAS'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "outliers = google_ads_data[(google_ads_data['ROAS'] < lower_bound) | (google_ads_data['ROAS'] > upper_bound)]\n",
    "print(f\"\\nNumber of outliers in Google Ads ROAS: {len(outliers)}\")\n",
    "print(\"Sample of outliers:\")\n",
    "print(outliers[['Date', 'ROAS', 'CVR', 'Revenue_per_Click', 'Impressions']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ece4a-a0d7-41b9-9263-14a2eda3ad71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
